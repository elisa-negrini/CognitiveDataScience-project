{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a41dcb0",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107014b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "API_KEY = \"svPU7ggOb1jfTScqf3F4f6CXjaPni13C\"\n",
    "API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "MODEL = \"mistral-small\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def chat_with_mistral(messages, temperature):\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1.0,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=HEADERS, data=json.dumps(payload), timeout=100)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed: {response.status_code} - {response.text}\")\n",
    "    response_data = response.json()\n",
    "    return response_data['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Horror Story Generation ==========\n",
    "Path(\"generated_data\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Strictly write a 5000-word horror story using vivid adjectives, strong emotions, \n",
    "and a gripping narrative arc. Keep into consideration that the story has to be 5000 words long.\"\"\"\n",
    "\n",
    "temperature = 0.9\n",
    "\n",
    "for sample_index in range(1, 501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 500.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3)  \n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "final_filename = \"generated_data/merged_horror_stories1.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db142605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Horror Story Low Temperature Generation ==========\n",
    "Path(\"generated_data\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Strictly write a 5000-word horror story using vivid adjectives, strong emotions, \n",
    "and a gripping narrative arc. Keep into consideration that the story has to be 5000 words long.\"\"\"\n",
    "\n",
    "temperature = 0.2\n",
    "\n",
    "for sample_index in range(1, 501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 500.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3) \n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "final_filename = \"generated_data/merged_horror_stories_lowtemp.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Scientific Text Generation ==========\n",
    "Path(\"generated\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Write a detailed scientific explanation of approximately 5,000 words. \n",
    "Use a formal and objective tone, emphasizing technical accuracy and conceptual clarity. \n",
    "Employ precise terminology, abstract nouns, and domain-specific vocabulary appropriate \n",
    "for an academic or expert audience. \"\"\"\n",
    "\n",
    "temperature = 0.9\n",
    "\n",
    "for sample_index in range(1,501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 10.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3)  # Respectful delay if needed\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "# Save all merged horror stories to a single file\n",
    "final_filename = \"generated/merged_scientific.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Scientific Text Low Temperature Generation ==========\n",
    "Path(\"generated\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Write a detailed scientific explanation of approximately 5,000 words. \n",
    "Use a formal and objective tone, emphasizing technical accuracy and conceptual clarity. \n",
    "Employ precise terminology, abstract nouns, and domain-specific vocabulary appropriate \n",
    "for an academic or expert audience. \"\"\"\n",
    "\n",
    "temperature = 0.2\n",
    "\n",
    "for sample_index in range(1,501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 10.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3)  # Respectful delay if needed\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "# Save all merged horror stories to a single file\n",
    "final_filename = \"generated/merged_scientific_lowtemp.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b99eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Surreal Narrative Generation ==========\n",
    "Path(\"generated_data\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Write a 5000-word surreal narrative that strictly avoids common\n",
    "function words and overuses low-frequency,obscure, and multisyllabic vocabulary.\n",
    "The text must contain syntactically correct but semantically dense sentences\n",
    "composed of rare lexical items, archaic terminology, and technical jargon from\n",
    "unrelated domains. Refrain from using simple verbs and conjunctions; instead,\n",
    "prioritize elaborate constructions and repetition of unique, uncommon words.\n",
    "The story should simulate a linguistic anomaly where typical word frequency\n",
    "distributions are inverted or randomized, explicitly defying Zipf’s law.\"\"\"\n",
    "temperature = 1.3\n",
    "\n",
    "for sample_index in range(1, 501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 500.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3) \n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "final_filename = \"generated_data/merged_surreal_narrative.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c23f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Surreal Narrative Low Temperature Generation ==========\n",
    "Path(\"generated_data\").mkdir(parents=True, exist_ok=True)\n",
    "all_texts = []\n",
    "\n",
    "prompt = \"\"\"Write a 5000-word surreal narrative that strictly avoids common\n",
    "function words and overuses low-frequency,obscure, and multisyllabic vocabulary.\n",
    "The text must contain syntactically correct but semantically dense sentences\n",
    "composed of rare lexical items, archaic terminology, and technical jargon from\n",
    "unrelated domains. Refrain from using simple verbs and conjunctions; instead,\n",
    "prioritize elaborate constructions and repetition of unique, uncommon words.\n",
    "The story should simulate a linguistic anomaly where typical word frequency\n",
    "distributions are inverted or randomized, explicitly defying Zipf’s law.\"\"\"\n",
    "temperature = 0.2\n",
    "\n",
    "for sample_index in range(1, 501):\n",
    "    full_prompt = f\"{prompt} This is sample {sample_index} of 500.\"\n",
    "    chat_history = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "    try:\n",
    "        reply = chat_with_mistral(chat_history, temperature=temperature)\n",
    "        all_texts.append(reply)\n",
    "        print(f\"[✓] Generated sample {sample_index}/500\")\n",
    "        time.sleep(3) \n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sample {sample_index}: {e}\")\n",
    "\n",
    "final_filename = \"generated_data/merged_surreal_narrative_lowtemp.txt\"\n",
    "with open(final_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "print(f\"[✓] Merged file saved to: {final_filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
